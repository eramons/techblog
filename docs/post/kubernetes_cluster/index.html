<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="E. Ramon">
  <meta name="description" content="Alles wird gut">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/techblog/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.0/css/academicons.min.css" integrity="sha512-GGGNUPDhnG8LEAEDsjqYIQns+Gu8RBs4j5XGlxl7UfRaZBhCCm5jenJkeJL8uPuOXGqgl8/H1gjlWQDRjd3cUQ==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/techblog/css/hugo-academic.css">
  

  

  <link rel="alternate" href="https://eramons.github.io/techblog/index.xml" type="application/rss+xml" title="Small Technical Blog">
  <link rel="feed" href="https://eramons.github.io/techblog/index.xml" type="application/rss+xml" title="Small Technical Blog">

  <link rel="icon" type="image/png" href="/techblog/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/techblog/img/apple-touch-icon.png">

  <link rel="canonical" href="https://eramons.github.io/techblog/post/kubernetes_cluster/">

  

  <title>Home-made K8s Cluster | Small Technical Blog</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/techblog/">Small Technical Blog</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/techblog/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/techblog/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/techblog/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Home-made K8s Cluster</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2020-03-21 15:00:00 &#43;0200 &#43;0200" itemprop="datePublished">
      Sat, Mar 21, 2020
    </time>
  </span>

  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2feramons.github.io%2ftechblog%2fpost%2fkubernetes_cluster%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Home-made%20K8s%20Cluster&amp;url=https%3a%2f%2feramons.github.io%2ftechblog%2fpost%2fkubernetes_cluster%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2feramons.github.io%2ftechblog%2fpost%2fkubernetes_cluster%2f&amp;title=Home-made%20K8s%20Cluster"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2feramons.github.io%2ftechblog%2fpost%2fkubernetes_cluster%2f&amp;title=Home-made%20K8s%20Cluster"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Home-made%20K8s%20Cluster&amp;body=https%3a%2f%2feramons.github.io%2ftechblog%2fpost%2fkubernetes_cluster%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<p><strong>Goal:</strong></p>

<p><em>Set up a home Kubernetes cluster with old hardware: 1 Master and 1 Worker</em></p>

<p>Tasks:</p>

<ol>
<li>Find old hardware</li>
<li>Install Ubuntu Server 18.04 LTS</li>
<li>Set up Master Node</li>
<li>Set up Worker Node</li>
<li>Service Account</li>
<li>Install helm</li>
<li>Install ingress</li>
<li>Install cert-manager</li>
<li>Cluster Issuer (Let&rsquo;s Encrypt)</li>
<li>Storage</li>
</ol>

<h2 id="1-find-old-hardware">1. Find old hardware</h2>

<p>Since kubernetes is lightweight and can run almost everywhere, I decided to go down to the cellar and rescue some old PC which I thought could still work for this. That&rsquo;s what I found:</p>

<ul>
<li>A MAC Mini Server from year 2009. Two 500GB disks. A still working grub bootloader told me I had a MacOS, an Ubuntu and two Debian distributions installed on the machine. This computer had 4GB RAM. I removed one of the disks, which was broken. One 500GB disk is more than enough for the Master anyway.</li>
<li>A PC tower whose components were bought separately and which I proudly assembled myself in 2007. Refurbished through the years, this PC had 6GB RAM and three hard disks: 250GB, 500GB and 2TB. Over the two first disks I had a LVM system installed, with a Debian installation and two separated home partitions. The PC didn&rsquo;t boot anymore, since the first disk - hosting the Master Boot Record - was broken. I removed the broken disk and the second LVM disk, keeping only the 2TB disk on the machine.</li>
</ul>

<p>So I thought the MAC Mini should be my Master and the PC Tower should be my Worker :)</p>

<p><img src="/techblog/img/kubehomecluster.jpg" alt="Home Kubernetes Cluster" /></p>

<h2 id="2-install-ubuntu-server-lts">2. Install Ubuntu Server LTS</h2>

<p>I decided to install the newest Ubuntu Server version with Long Term Support (LTS) on both machines. An alternative could have been to install the latest Debian stable.</p>

<p>Download Ubuntu Server 18.04.4 LTS from <a href="https://ubuntu.com/download/server" target="_blank">https://ubuntu.com/download/server</a></p>

<p>For the MacMini, I just created a bootable USB.</p>

<p>Insert an USB drive and find out which device is it mapped to:</p>

<pre><code>sudo dmesg |grep sd
...
[24400.755280] sd 4:0:0:0: [sdb] Attached SCSI disk
[24406.001628] EXT4-fs (sdb1): mounted filesystem with ordered data mode. Opts: (null)
[39200.249434] sd 4:0:0:0: [sdb] 31266816 512-byte logical blocks: (16.0 GB/14.9 GiB)
...
</code></pre>

<p>It was /dev/sdb. So I copied the downloaded image to the USB drive:</p>

<pre><code>sudo dd if=ubuntu-18.04.4-live-server-amd64.iso of=/dev/sdb
</code></pre>

<p>For the old PC, I had to burn a DVD, since it was so old it did not even boot from USB. For that I borrowed a quite old Windows laptop which still had a DVD-drive. I burned the ISO image using the Windows Disc Image Burner.</p>

<p>After having the media prepared, I installed Ubuntu Server on both machines. In both cases, I chose &ldquo;Manual&rdquo; for setting up the disks and the partitions, using the whole disk.</p>

<p>NOTE: swap is not supported. Either it must be omitted during installation, or it must be switched off afterwards:</p>

<pre><code>sudo swapoff -a 
</code></pre>

<h2 id="3-master-node">3. Master node</h2>

<h3 id="3-1-install-runtime">3.1. Install runtime</h3>

<p>I searched for a suitable container runtime to install, gettin two results which made sense for me:</p>

<pre><code>eramon@caipirinha:~/dev/techblog$ apt-cache search docker.io
containerd - open and reliable container runtime
docker.io - Linux container runtime
...
</code></pre>

<p>Although containerd would be enough, I installed docker.io since having the extra docker tools for debugging or whatever might be useful later:</p>

<pre><code>sudo apt-get install docker.io
</code></pre>

<h3 id="3-2-install-kubernetes-software">3.2. Install kubernetes software</h3>

<p>Install kubeadm, kubectl and kubelet</p>

<pre><code>sudo apt-get update
sudo apt-get install -y apt-transport-https curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo vi /etc/apt/sources.list.d/kubernetes.list
Add:
deb https://apt.kubernetes.io/ kubernetes-xenial main (probably not good, I have bionic) There is no kubernetes-bionic under packages.cloud.google.com/apt/dists
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre>

<p>kubelet, kubeadm and kubectl are set on hold (to avoid automatic updates, I think)</p>

<p>After installation, restarting kubelet is required;</p>

<pre><code>sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>

<h3 id="3-3-initialise-master-node">3.3 Initialise master node</h3>

<p>The goal was to set up a single control-plane cluster with kubeadm.</p>

<pre><code>sudo kubeadm init --pod-network-cidr=10.244.0.0/16
...

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.129:6443 --token jn333p.q9hskm01cs12iak7 \
    --discovery-token-ca-cert-hash sha256:0966963ed31ac9d898e3d49d154e2f6ed78931f356af5d6c35616ee75585c2f9

</code></pre>

<p>To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output:</p>

<pre><code>eramon@pacharan:~$ sudo mkdir -p $HOME/.kube
eramon@pacharan:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
eramon@pacharan:~$ sudo chown eramon:eramon .kube/config
</code></pre>

<p>After these steps, looking at the output of kubectl cluster-info we see kubectl is interacting with our new cluster:</p>

<pre><code>eramon@pacharan:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.1.129:6443
KubeDNS is running at https://192.168.1.129:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<h3 id="3-4-install-a-pod-network-add-on">3.4 Install a Pod network add-on</h3>

<p>As the output of kubeadm said, we must deploy a pod network to the cluster, so the pods can communicate with each other.</p>

<pre><code>eramon@pacharan:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
</code></pre>

<p>Show all pods from all namespaces:</p>

<pre><code>eramon@pacharan:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-6955765f44-597bj           1/1     Running   0          160m
kube-system   coredns-6955765f44-ctwvs           1/1     Running   0          160m
kube-system   etcd-pacharan                      1/1     Running   0          160m
kube-system   kube-apiserver-pacharan            1/1     Running   0          160m
kube-system   kube-controller-manager-pacharan   1/1     Running   0          160m
kube-system   kube-flannel-ds-amd64-dhc7f        1/1     Running   0          62s
kube-system   kube-proxy-qcv98                   1/1     Running   0          160m
kube-system   kube-scheduler-pacharan            1/1     Running   0          160m
</code></pre>

<p>Flannel was there.</p>

<h2 id="4-worker-node">4. Worker node</h2>

<h3 id="4-1-installation">4.1 Installation</h3>

<p>I followed exactly the same process described in 3.1 and 3.2 to install the container runtime and the kubernetes software on the worker node.</p>

<h3 id="4-2-join-the-cluster">4.2 Join the cluster</h3>

<p>First enable docker.service (otherwise kubeadm join shows a warning):</p>

<pre><code>eramon@whisky:~$ sudo systemctl enable docker.service
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.
</code></pre>

<p>Since more than 24 hours elapsed between the kubeadm init and the join command, I created a new token:</p>

<pre><code>eramon@pacharan:~$ kubeadm token create
W0321 15:00:44.641034   18998 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0321 15:00:44.641111   18998 validation.go:28] Cannot validate kubelet config - no validator is available
qon04q.9lwkz7i4pixr46q6
eramon@pacharan:~$ kubeadm token list
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
qon04q.9lwkz7i4pixr46q6   23h         2020-03-22T15:00:44Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token
</code></pre>

<p>Join worker node by running the following as root:</p>

<pre><code>root@whisky:/home/eramon# kubeadm join 192.168.1.129:6443 --token qon04q.9lwkz7i4pixr46q6 --discovery-token-ca-cert-hash sha256:0966963ed31ac9d898e3d49d154e2f6ed78931f356af5d6c35616ee75585c2f9
W0321 15:03:52.406236   29903 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.17&quot; ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;
[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
</code></pre>

<p>As instructed by the output of the last command, we take a look at the nodes:</p>

<pre><code>eramon@pacharan:~$ kubectl get nodes
NAME       STATUS   ROLES    AGE     VERSION
pacharan   Ready    master   3d3h    v1.17.4
whisky     Ready    &lt;none&gt;   6m15s   v1.17.4
</code></pre>

<h2 id="5-service-account">5. Service Account</h2>

<p>To interact with the Kubernetes cluster from a client machine, for now we&rsquo;ll use the default service account token.</p>

<p>Service accounts are users managed by the Kubernetes cluster and tied to a set of credentials stored as Secrets, which are mounted into pods allowing in-cluster processes to talk to the Kubernetes API.</p>

<h3 id="5-1-install-kubectl">5.1. Install kubectl</h3>

<p><em>On the client machine</em></p>

<p>Download binary and install kubectl on the client machine:</p>

<pre><code>eramon@caipirinha:~/dev/kubernetes$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 41.4M  100 41.4M    0     0  5681k      0  0:00:07  0:00:07 --:--:-- 5877k

eramon@caipirinha:~/dev/kubernetes$ chmod +x kubectl 

eramon@caipirinha:~/dev/kubernetes$ sudo mv kubectl /usr/local/bin/kubectl

</code></pre>

<h3 id="5-2-grant-access-to-the-cluster-api-for-the-client-machine">5.2. Grant access to the cluster API for the client machine</h3>

<p>I just copied the kube configuration from the Master node to my laptop.</p>

<p><em>NOTE: Not sure if this is exactly best practice.</em></p>

<p>The file is located here:</p>

<pre><code>eramon@pacharan:~$ ls -la /home/eramon/.kube/config 
-rw------- 1 eramon eramon 5449 Mar 18 14:42 /home/eramon/.kube/config
</code></pre>

<p>After copying the file to the same location on the client machine, I saw my laptop had access to the Kubernetes API on the server via kubectl:</p>

<pre><code>eramon@caipirinha:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.1.129:6443
KubeDNS is running at https://192.168.1.129:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
</code></pre>

<h2 id="6-install-helm">6. Install helm</h2>

<p>Helm is a tool for managing Kubernetes packages called <em>charts</em>. The chart is a bundle of information necessary to create an instance of a Kubernetes application.</p>

<p>Downloaded latest helm:</p>

<pre><code>eramon@caipirinha:~/dev/helm$ wget https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz
--2020-03-22 13:12:45--  https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz
Resolving get.helm.sh (get.helm.sh)... 2606:2800:233:1cb7:261b:1f9c:2074:3c, 152.199.21.175
Connecting to get.helm.sh (get.helm.sh)|2606:2800:233:1cb7:261b:1f9c:2074:3c|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 12269190 (12M) [application/x-tar]
Saving to: ‘helm-v3.1.2-linux-amd64.tar.gz’

helm-v3.1.2-linux-amd64. 100%[==================================&gt;]  11.70M  2.53MB/s    in 4.6s

2020-03-22 13:12:51 (2.53 MB/s) - ‘helm-v3.1.2-linux-amd64.tar.gz’ saved [12269190/12269190]
</code></pre>

<p>Unzip it and instal binary:</p>

<pre><code>eramon@caipirinha:~/dev/helm$ tar -zxvf helm-v3.1.2-linux-amd64.tar.gz
linux-amd64/
linux-amd64/helm
linux-amd64/README.md
linux-amd64/LICENSE

eramon@caipirinha:~/dev/helm$ sudo mv linux-amd64/helm /usr/local/bin/helm
</code></pre>

<p>Invoke helm to make sure it&rsquo;s working:</p>

<pre><code>eramon@caipirinha:~$ helm version
version.BuildInfo{Version:&quot;v3.1.2&quot;, GitCommit:&quot;d878d4d45863e42fd5cff6743294a11d28a9abce&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.13.8&quot;}
</code></pre>

<p>Once Helm is ready, add the official &ldquo;Helm stable charts&rdquo; chart repository:</p>

<pre><code>eramon@caipirinha:~$ helm repo add stable https://kubernetes-charts.storage.googleapis.com/
&quot;stable&quot; has been added to your repositories
</code></pre>

<h2 id="7-install-nginx-ingress">7 Install nginx ingress</h2>

<p>My cluster needed an ingress controller in order to listen and serve connection requests.</p>

<p>Since I&rsquo;m working with a home-made cluster installed directly on physical machines running on my home network, the default nginx-ingress configuration won&rsquo;t work for me. On a home configuration there is no load balancer on-demand as with a cloud provider.</p>

<p>Taking a &ldquo;bare-metal&rsquo; cluster configuration as a reference, this is the setup for my ingress controller:</p>

<ul>
<li><em>kind=DaemonSet</em>: deploy the ingress controler on every node</li>
<li><em>hostNetwork=true</em>: the nginx daemonset runs on the host namespace</li>
<li><em>service.enabled=false</em>: no service will be created, since we are using DaemonSet</li>
<li><em>admisionWebhooks.enabled=false</em>: we don&rsquo;t want to have ingress admission webhooks</li>
</ul>

<p>Install nginx-ingress using helm:</p>

<pre><code>helm install mynginx1 stable/nginx-ingress \
	--set controller.admisionWebhooks.enabled=false \
	--set controller.service.enabled=false \
	--set controller.hostNetwork=true \
        --set controller.kind=DaemonSet
</code></pre>

<h2 id="8-install-cert-manager">8 Install cert-manager</h2>

<p>The issuance, renewal and configuration of TLS server certificates for web applications deployed in the cluster can be automated with Let&rsquo;s Encrypt and cert-manager.</p>

<p>Install cert-manager from manifest (rather as using helm):</p>

<pre><code>eramon@caipirinha:~/dev/kubernetes$ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.3/cert-manager.yaml
</code></pre>

<p>The cert-manager is installed in an own new namespace called <em>cert-manager</em>.</p>

<p>To make sure cert-manager was installed correctly, list the pods on the namespace:</p>

<pre><code>eramon@caipirinha:~/dev/techblog$ kubectl get pods -n cert-manager
NAME                                      READY   STATUS    RESTARTS   AGE
cert-manager-77fcb5c94-dfgjm              1/1     Running   0          58m
cert-manager-cainjector-8fff85c75-nmmhn   1/1     Running   0          58m
cert-manager-webhook-54647cbd5-w4fkr      1/1     Running   0          58m
</code></pre>

<p>If the cert-manager, the cainjector and the webhook are up and running, we should be good.</p>

<h2 id="9-clusterissuer">9 ClusterIssuer</h2>

<p><strong>Issuers</strong> (and <strong>ClusterIssuers</strong>) represent a certificate authority from which signed x509 certificates can be obtained. A ClusterIssuer is necessary to issue certificates with cert-manager and Let&rsquo;s Encrypt.</p>

<p>I used a DNS01 challenge with an api token provided by Cloudflare.</p>

<p><em>NOTE: A DNS01 solver with acme version 2 is mandatory for issuance of wildcard certificates.</em></p>

<p>ClusterIssuer: <a href="https://github.com/eramons/kubecozy/blob/master/letsencrypt-prod.yaml.example" target="_blank">letsencrypt-prod.yaml.example</a></p>

<h2 id="10-storage">10 Storage</h2>

<p>There are many different kinds of persistent volumes for K8s. One of them is a NFS server.</p>

<h3 id="10-1-set-up-a-nfs-share">10.1 Set up a NFS share</h3>

<p>Setting up the NFS share on the Synology NAS was quite straightforward.</p>

<h3 id="10-2-install-nfs-common">10.2 Install nfs-common</h3>

<p>I installed nfs-common on my worker node:</p>

<pre><code>sudo apt-get install nfs-common
</code></pre>

<p>With this, the worker will be able to nfs-mount folders on my NAS as persistent volumes for the pods.</p>

<p><em>Note: not sure if this is best practice. I guess for productive setups required software is installed on each node automatically.</em></p>

<h3 id="10-3-persistent-volume">10.3 Persistent Volume</h3>

<p>After the NFS share was available on my NAS, I wrote manifests for persistent nfs volumes. I created a dedicated persistent volume for each application.</p>

<p>The manifest reference the hostname or IP of the NFS server and the path to the share:</p>

<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: synology-nfs
spec:
  capacity:
    storage: 10Gi
  storageClassName: standard
  accessModes:
  - ReadWriteMany
  nfs:
    server: myServerIP
    path: /path/to/share
</code></pre>

<p>After all these preparations, my new cluster was ready for its first deployment :)</p>

<h2 id="appendix-open-points">Appendix. Open points:</h2>

<h3 id="warning">Warning</h3>

<p>In both master and worker, I get this warning:</p>

<pre><code>[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/
</code></pre>

<h3 id="warning-1">Warning</h3>

<p>When running kubeadm init and when creating a new token, I got warnings on the output:</p>

<pre><code>eramon@pacharan:~$ kubeadm token create
W0321 15:16:13.144929   25354 validation.go:28] Cannot validate kube-proxy config - no validator is available
W0321 15:16:13.145003   25354 validation.go:28] Cannot validate kubelet config - no validator is available
4w2xrh.i1phnkxkzz7ja5fi
</code></pre>

<h2 id="references-and-useful-or-interesting-links">References and useful or interesting links:</h2>

<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm" target="_blank">Install kubeadm</a></p>

<p><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">Install and Set Up kubectl</a></p>

<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm" target="_blank">Create single-control node cluster with kubeadm</a></p>

<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network" target="_blank">Install pod network</a></p>

<p><a href="https://github.com/coreos/flannel" target="_blank">Flannel</a></p>

<p><a href="https://helm.sh/docs/intro/" target="_blank">Introduction to Helm</a></p>

<p><a href="https://github.com/helm/helm/releases" target="_blank">Helm Releases @Github</a></p>

<p><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">nginx-ingress</a></p>

<p><a href="https://github.com/helm/charts/tree/master/stable/nginx-ingress" target="_blank">nginx helm chart</a></p>

<p><a href="https://cert-manager.io/docs/installation/kubernetes" target="_blank">cert-manager</a></p>

<p><a href="https://cert-manager.io/docs/configuration/acme/dns01/cloudflare" target="_blank">Cloudflare DNS01 solver</a></p>

    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://eramons.github.io/techblog/post/debug_coreboot/"><span
      aria-hidden="true">&larr;</span> EHCI Debug</a></li>
    

    
    <li class="next"><a href="https://eramons.github.io/techblog/post/dns/">DNS Configuration for K8s <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2020 E. Ramon &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.2/imagesloaded.pkgd.min.js" integrity="sha512-iHzEu7GbSc705hE2skyH6/AlTpOfBmkx7nUqTLGzPYR+C1tRaItbRlJ7hT/D3YQ9SV0fqLKzp4XY9wKulTBGTw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js" integrity="sha512-Z5heTz36xTemt1TbtbfXtTq5lMfYnOkXM2/eWcTTiLU01+Sw4ku1i7vScDc8fWhrP2abz9GQzgKH5NGBLoYlAw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/plugins/ScrollToPlugin.min.js" integrity="sha512-CDeU7pRtkPX6XJtF/gcFWlEwyaX7mcAp5sO3VIu/ylsdR74wEw4wmBpD5yYTrmMAiAboi9thyBUr1vXRPA7t0Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="/techblog/js/hugo-academic.js"></script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

